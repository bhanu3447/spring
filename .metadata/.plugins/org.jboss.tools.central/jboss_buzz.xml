<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How to create execution environments using ansible-builder</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/08/how-create-execution-environments-using-ansible-builder" /><author><name>Tathagata Paul</name></author><id>7453d38e-90f7-4d30-9fba-9db9e244053d</id><updated>2023-05-08T07:00:00Z</updated><published>2023-05-08T07:00:00Z</published><summary type="html">&lt;p&gt;The execution environment builder (aka Ansible Builder) is a part of &lt;a href="https://developers.redhat.com/products/ansible/"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;. It is a command-line interface (CLI) tool for building and creating custom execution environments. The Ansible Builder project enables users to automate and accelerate the process of creating execution environments. This article will show you how to install and use the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/2.1/html/ansible_builder_guide/index"&gt;execution environment builder&lt;/a&gt; CLI tool.&lt;/p&gt; &lt;h2&gt;Installing the execution environment builder&lt;/h2&gt; &lt;p&gt;The execution environment builder makes it easier for Ansible Automation Platform content creators and administrators to build custom execution environments. They can use dependency information from various &lt;a href="http://ansible.com/products/content-collections"&gt;Ansible Content Collections&lt;/a&gt; and directly from the user.&lt;/p&gt; &lt;h3&gt;Step 1: Install the execution environment builder tool&lt;/h3&gt; &lt;p&gt;Install the execution environment builder tool from the Python Package Index (PyPI) by using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;pip install ansible-builder&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 2: Access the ansible-builder subcommands&lt;/h3&gt; &lt;p&gt;To access the subcommands of &lt;code&gt;ansible-builder&lt;/code&gt;, run &lt;code&gt;build&lt;/code&gt; and &lt;code&gt;create&lt;/code&gt; commands to get help output.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;build&lt;/code&gt; subcommand will build the execution environment using the definition file.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-builder build –help&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It populates the build context and then uses Podman or Docker to create the execution environment image. The help output appears as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;usage: ansible-builder build [-h] [-t TAG] [--container-runtime {podman,docker}] [--build-arg BUILD_ARGS] [-f FILENAME] [-c BUILD_CONTEXT] [--output-filename {Containerfile,Dockerfile}] [-v {0,1,2,3}] Creates a build context (including a Containerfile) from an execution environment spec. The build context will be populated from the execution environment spec. After that, the specified container runtime podman/docker will be invoked to build an image from that definition. After building the image, it can be used locally or published using the supplied tag. optional arguments: -h, --help show this help message and exit -t TAG, --tag TAG The name for the container image being built (default: ansible-execution-env:latest) --container-runtime {podman,docker} Specifies which container runtime to use (default: podman) --build-arg BUILD_ARGS Build-time variables to pass to any podman or docker calls. Internally ansible-builder makes use of ANSIBLE_GALAXY_CLI_COLLECTION_OPTS, EE_BASE_IMAGE, EE_BUILDER_IMAGE. -f FILENAME, --file FILENAME The definition of the execution environment (default: execution-environment.yml) -c BUILD_CONTEXT, --context BUILD_CONTEXT The directory to use for the build context (default: context) --output-filename {Containerfile,Dockerfile} Name of file to write image definition to (default depends on --container-runtime, Containerfile for podman and Dockerfile for docker) -v {0,1,2,3}, --verbosity {0,1,2,3} Increase the output verbosity, for up to three levels of verbosity (invoked via "--verbosity" or "-v" followed by an integer ranging in value from 0 to 3) (default: 2) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;create&lt;/code&gt; subcommand works similar to the &lt;code&gt;build&lt;/code&gt; command.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-builder create –help&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, it will not build the execution environment image as you will see in the following output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;usage: ansible-builder build [-h] [-t TAG] [--container-runtime {podman,docker}] [--build-arg BUILD_ARGS] [-f FILENAME] [-c BUILD_CONTEXT] [--output-filename {Containerfile,Dockerfile}] [-v {0,1,2,3}] Creates a build context (including a Containerfile) from an execution environment spec. The build context will be populated from the execution environment spec. After that, the specified container runtime podman/docker will be invoked to build an image from that definition. After building the image, it can be used locally or published using the supplied tag. optional arguments: -h, --help show this help message and exit -t TAG, --tag TAG The name for the container image being built (default: ansible-execution-env:latest) --container-runtime {podman,docker} Specifies which container runtime to use (default: podman) --build-arg BUILD_ARGS Build-time variables to pass to any podman or docker calls. Internally ansible-builder makes use of ANSIBLE_GALAXY_CLI_COLLECTION_OPTS, EE_BASE_IMAGE, EE_BUILDER_IMAGE. -f FILENAME, --file FILENAME The definition of the execution environment (default: execution-environment.yml) -c BUILD_CONTEXT, --context BUILD_CONTEXT The directory to use for the build context (default: context) --output-filename {Containerfile,Dockerfile} Name of file to write image definition to (default depends on --container-runtime, Containerfile for podman and Dockerfile for docker) -v {0,1,2,3}, --verbosity {0,1,2,3} Increase the output verbosity, for up to three levels of verbosity (invoked via "--verbosity" or "-v" followed by an integer ranging in value from 0 to 3) (default: 2) &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 3: Populate the ansible-builder spec&lt;/h3&gt; &lt;p&gt;Populate the &lt;code&gt;ansible-builder&lt;/code&gt; spec to build the custom execution environment by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mkdir project_directory &amp;&amp; cd project_directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Populate the &lt;code&gt;execution-environment.yml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; execution-environment.yml --- version: 1 dependencies: galaxy: requirements.yml EOT &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a &lt;code&gt;requirements.yml&lt;/code&gt; file and populate the contents with the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; requirements.yml --- collections: - name: servicenow.itsm EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Through the spec and requirements file, we ensure that execution environment builder will download the &lt;strong&gt;servicenow.itsm collection&lt;/strong&gt; while building the execution environment. The default download location is &lt;strong&gt;galaxy.ansible.com&lt;/strong&gt;. You can also point to an automation hub or your own hub instance in the spec file.&lt;/p&gt; &lt;h3&gt;Step 4: Build the execution environment&lt;/h3&gt; &lt;p&gt;Build the execution environment using the previously created files. Run the following command to create a new custom execution environment called &lt;strong&gt;custom-ee:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-builder build -v3 -t custom-ee&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;-v3&lt;/code&gt; flag adds verbosity to the CLI run, and &lt;code&gt;-t custom-ee&lt;/code&gt; will tag your image with the name you provided.&lt;/p&gt; &lt;p&gt;The output appears as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Ansible Builder is building your execution environment image, "custom-ee". File context/_build/requirements.yml will be created. Rewriting Containerfile to capture collection requirements Running command: podman build -f context/Containerfile -t custom-ee context [1/3] STEP 1/7: FROM registry.redhat.io/ansible-automation-platform-21/ee-minimal-rhel8:latest AS galaxy [1/3] STEP 2/7: ARG ANSIBLE_GALAXY_CLI_COLLECTION_OPTS= --&gt; 88d9ea223d0 [1/3] STEP 3/7: USER root --&gt; 549f29055c2 [1/3] STEP 4/7: ADD _build /build --&gt; 0d3e9515b12 [1/3] STEP 5/7: WORKDIR /build --&gt; 3b290acf78c [1/3] STEP 6/7: RUN ansible-galaxy role install -r requirements.yml --roles-path /usr/share/ansible/roles Skipping install, no requirements found --&gt; 8af36370e78 [1/3] STEP 7/7: RUN ansible-galaxy collection install $ANSIBLE_GALAXY_CLI_COLLECTION_OPTS -r requirements.yml --collections-path /usr/share/ansible/collections Starting galaxy collection install process Process install dependency map … &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the following commands to check the image list:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman images&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output appears as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;REPOSITORY TAG IMAGE ID CREATED SIZE localhost/custom-ee latest bfe6c40bad52 21 seconds ago 626 MB &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 5: Build a complex execution environment&lt;/h3&gt; &lt;p&gt;To build a complex execution environment, go back into the project directory with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd project_directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Edit the &lt;code&gt;execution-environment.yml&lt;/code&gt; file and add the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; execution-environment.yml --- version: 1 dependencies: galaxy: requirements.yml python: requirements.txt system: bindep.txt additional_build_steps: prepend: | RUN whoami RUN cat /etc/os-release append: - RUN echo This is a post-install command! - RUN ls -la /etc EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can see the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Python requirements were added through the &lt;strong&gt;requirements.txt&lt;/strong&gt; file, which will hold the pip dependencies.&lt;/li&gt; &lt;li&gt;We added a &lt;strong&gt;bindep.txt&lt;/strong&gt;, which will hold the rpm installs.&lt;/li&gt; &lt;li&gt;Additional build steps that will run before (prepend) and after (append) the build steps.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Now create a new file called &lt;code&gt;requirements.yml&lt;/code&gt; and append the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; requirements.yml --- collections: - name: servicenow.itsm - name: ansible.utils EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We added a new collection called &lt;strong&gt;ansible.utils&lt;/strong&gt; alongside the &lt;strong&gt;servicenow.itsm&lt;/strong&gt; file.&lt;/p&gt; &lt;p&gt;Create a new file called &lt;code&gt;requirements.txt&lt;/code&gt; and then append the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; requirements.txt gcp-cli ncclient netaddr paramiko EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This contains the &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; requirements that need to be installed via pip.&lt;/p&gt; &lt;p&gt;Create a new file called &lt;code&gt;bindep.txt&lt;/code&gt; and then append the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; bindep.txt findutils [compile platform:centos-8 platform:rhel-8] gcc [compile platform:centos-8 platform:rhel-8] make [compile platform:centos-8 platform:rhel-8] python38-devel [compile platform:centos-8 platform:rhel-8] python38-cffi [platform:centos-8 platform:rhel-8] python38-cryptography [platform:centos-8 platform:rhel-8] python38-pycparser [platform:centos-8 platform:rhel-8] EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This file contains the rpm requirements needed to be installed using dnf.&lt;/p&gt; &lt;p&gt;Run the following build:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-builder build -v3 -t custom-ee&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output is as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Ansible Builder is building your execution environment image, "custom-ee". File context/_build/requirements.yml will be created. File context/_build/requirements.txt will be created. File context/_build/bindep.txt will be created. Rewriting Containerfile to capture collection requirements Running command: podman build -f context/Containerfile -t custom-ee context [1/3] STEP 1/7: FROM registry.redhat.io/ansible-automation-platform-21/ee-minimal-rhel8:latest AS galaxy [1/3] STEP 2/7: ARG ANSIBLE_GALAXY_CLI_COLLECTION_OPTS= --&gt; Using cache 88d9ea223d01bec0d53eb7efcf0e76b5f7da0285a411f2ce0116fe9641cbc3a0 --&gt; 88d9ea223d0 [1/3] STEP 3/7: USER root --&gt; Using cache 549f29055c2f1ba0ef3f7c5dfdc67a40302ff0330af927adb94fbcd7b0b1e7b4 --&gt; 549f29055c2 [1/3] STEP 4/7: ADD _build /build --&gt; 6b9ee91e773 [1/3] STEP 5/7: WORKDIR /build --&gt; 5518e019f2d [1/3] STEP 6/7: RUN ansible-galaxy role install -r requirements.yml --roles-path /usr/share/ansible/roles Skipping install, no requirements found --&gt; 60c1605d66c [1/3] STEP 7/7: RUN ansible-galaxy collection install $ANSIBLE_GALAXY_CLI_COLLECTION_OPTS -r requirements.yml --collections-path /usr/share/ansible/collections Starting galaxy collection install process Process install dependency map&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check the context or &lt;code&gt;Containerfile&lt;/code&gt; to see all the steps you took to build the execution environment. You can transfer the context directory to a different server and replicate the image creation via &lt;code&gt;docker&lt;/code&gt; or &lt;code&gt;podman&lt;/code&gt; commands.&lt;/p&gt; &lt;h2&gt;Pushing the execution environment to a private automation hub&lt;/h2&gt; &lt;p&gt;Log in to the &lt;a href="https://www.ansible.com/blog/control-your-content-with-private-automation-hub"&gt;private automation hub&lt;/a&gt; by using the &lt;code&gt;podman&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman login &lt;automation hub url&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then tag the image before pushing it to the hub as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman tag localhost/custom-ee &lt;automation hub url&gt;/developers-bu-aap-builder&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, push it to the private automation hub as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman push &lt;automation hub url&gt;/developers-bu-aap-builder&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can see the image pushed to the private automation hub in Figure 1:&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image_1.png?itok=AxLHdHot" width="1440" height="816" alt="The private automation hub page showing multiple pushed execution environment images." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The private automation hub page showing multiple pushed execution environment images.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Continue your automation journey with Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Get started with Ansible Automation Platform&lt;/a&gt; by exploring &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;interactive labs&lt;/a&gt;. Check out Red Hat’s hands-on labs for all skill levels to learn more. The wide range of labs include &lt;a href="https://developers.redhat.com/learn/lessons/linux-commands?intcmp=7013a0000026UTXAA2"&gt;useful Linux commands&lt;/a&gt;, &lt;a href="https://developers.redhat.com/learn/installing-software-using-package-managers?intcmp=7013a0000026UTXAA2"&gt;Install software using package managers&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/learn/lessons/deploying-containers-podman?intcmp=7013a0000026UTXAA2"&gt;Deploying containers using container tools [podman]&lt;/a&gt;. Try these labs to see your favorite products in action. Ansible Automation Platform is also available as a managed offering on&lt;a href="https://www.redhat.com/en/technologies/management/ansible/azure"&gt; Microsoft Azure&lt;/a&gt; and as a self-managed offering on &lt;a href="https://www.redhat.com/en/technologies/management/ansible/aws"&gt;AWS&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/08/how-create-execution-environments-using-ansible-builder" title="How to create execution environments using ansible-builder"&gt;How to create execution environments using ansible-builder&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Tathagata Paul</dc:creator><dc:date>2023-05-08T07:00:00Z</dc:date></entry><entry><title type="html">How to run Spring Boot applications on WildFly</title><link rel="alternate" href="https://www.mastertheboss.com/jboss-frameworks/spring/spring-boot-hello-world-on-wildfly/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/jboss-frameworks/spring/spring-boot-hello-world-on-wildfly/</id><updated>2023-05-06T07:09:05Z</updated><content type="html">This updated (May 2023) article shows how to deploy Spring Boot 3 / Spring Boot 2 applications on top of WildFly application server as Web application archives (war). We will start by setting up the application with Spring Boot Initializr. Then, we will apply the configuration changes to deploy the application on WildFly. Spring Boot ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">What&amp;#8217;s new in Dashbuilder 0.28.0</title><link rel="alternate" href="https://blog.kie.org/2023/05/whats-new-in-dashbuilder-0-28-0.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2023/05/whats-new-in-dashbuilder-0-28-0.html</id><updated>2023-05-05T17:39:58Z</updated><content type="html">Kie Tools 0.28.0 is out and with it we have multiple improvements to Dashbuilder. Let’s explore it! INSTALLATION The new editor is live in and the VSCode extension is already available on the . For users just update the dashbuilder-client dependency version to  0.28.0. DASHBUILDER SAMPLES We organized Dashbuilder samples in the repository. Notice you can also access examples directly from . DASHBUILDER CLEANUP AND BACKEND REMOVAL Dashbuilder Authoring and Dashbuilder Runtime App were removed on this version. The reason is that Dashbuilder Runtime is now focused on YAML development and users who need a backend to produce datasets can make use of . As the consequence we removed 238k lines of code and made Dashbuilder faster and smaller: * Java classes from 3088 to 1546 * Dashbuilder client bundle from 21mb to 18mb * Main Javascript reduction from 1.8mb to 1.3mb (with gzip) * Main Load time reduction from ~1.1s to ~900ms ERROR MESSAGES AND USER FEEDBACK IMPROVEMENTS Continuing Dashbuilder user feedback improvements, my colleague Kumar Aditya made 3 great improvements: * Improve unreachable URLs error message.  * Improve dataset parsing error message * Show a message when a displayer configuration is invalid. Dashbuilder used to ignore the displayer with bad configuration, now it shows the cause for the bad configuration. DATASET IMPROVEMENTS Important improvements were made to Dashbuilder datasets * Dataset columns: Now dashbuilder set the LABEL as the default type for columns and it is able to retrieve columns name from Metrics or CSVs * Accumulate Flag: Datasets now can keep data in the memory if accumulate flag is true. This is especially important when reading metrics without Prometheus and having a displayer with auto update, then the metric values are kept in memory and you can display it any way you want. Learn more about it in the that introduced the accumulate flag * CSV parser: The CSV parser had minor fixes and the only currently known limitation is regarding quoted fields with line break.  CONCLUSION The 0.28.0 release is a great milestone for Dashbuilder! The backend removal was important not only for performance reasons, but also for code maintenance because now we have less classes to maintain and evolve 🙂  The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>Quarkus 2.16.7.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-16-7-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-16-7-final-released/</id><updated>2023-05-05T00:00:00Z</updated><published>2023-05-05T00:00:00Z</published><summary type="html">We released Quarkus 2.16.6.Final, the seventh maintenance release of our 2.16 release train. As usual, it contains bugfixes and documentation improvements. It should be a safe upgrade for anyone already using 2.16. If you are not already using 2.16, please refer to our migration guide. Full changelog You can get...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-05-05T00:00:00Z</dc:date></entry><entry><title type="html">Integrate Excel with Drools on OpenShift with Knative and Quarkus!</title><link rel="alternate" href="https://blog.kie.org/2023/05/integrate-excel-with-drools-on-openshift-with-knative-and-quarkus.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2023/05/integrate-excel-with-drools-on-openshift-with-knative-and-quarkus.html</id><updated>2023-05-04T08:42:59Z</updated><content type="html">In this blog post I want to share the results of a technical exploration in bridging, bringing together and integrating a diverse set of technologies and platforms, ranging from classic spreadsheet applications (Excel) to serverless platforms (Knative on OpenShift) to technical rules executed by our rule engine Drools! INTRODUCTION This content has been inspired by I had the opportunity to read recently (see more below). So I wanted to take on a personal challenge to build a novel example, based on some of the powerful techniques presented in the book, and then add some more, going beyond. Specifically, I wanted to be able to invoke some custom DRL rule evaluation in a serverless way, by connecting Excel with my -based application served by on . As I wanted a use-case with plenty of realistic data for this technical exploration, I decided to focus my attention on the IoT (Internet of Things) which is another factor revolutionizing the way we live. If I think about the diverse ranges of devices available nowadays, from smart homes to connected cars, these IoT devices in my opinion are changing not only the way we interact with our surroundings… when used sapiently, I believe they can really augment and improve our lives. However, IoT is more than just internet-connected devices! To me, it is also about leveraging various technologies and platforms to create intelligent systems that can automate processes, optimize, and improve our decision-making. More specifically, I wanted to try processing the technical data collected through my smart scale and smart watch, collecting into Excel, and then processing it via the intelligent application described above. This will give us the opportunity to highlight some of the benefits of the integration scope mentioned in the preamble, and a perspective on how these techniques can help your organization or benefit your own use-cases! Before wrapping up, I will share my review of the mentioned book. SERVERLESS DROOLS Let’s dive into the DRL rules: rule R1 when $r : MeasRecord( morning == true, bpm &lt; 60 || bpm &gt; 100 ) then insert(new Advice("abnormal Blood Pressure in the morning", 100)); end rule R2 when $r : MeasRecord( weight &lt; weight_m3, weight_m3 &lt; weight_m5 ) then insert(new Advice("downward trend in weight")); end rule R3 when $r : MeasRecord( weight &gt; weight_m3, weight_m3 &gt; weight_m5 ) then insert(new Advice("upward trend in weight")); end Here, I want to define some rules which will advise me if specific data measurement is observed. These rules in my opinion are very naturally readable in spite of the technical nature of DRL: I want to emit an advice in case of abnormal bpm, or when there is a specific trend in weight compared to T-3D or T-5D (I take these measurements once each day). Similarly, you could think of analogous DRL rules for your IoT use-case, reacting to events and measurement signals from your sensors or devices! In order to make this intelligent application efficiently consumable as a serverless decision service, I decided to experiment with a number of capabilities of Drools v8 and Quarkus, starting by making use of the Drools v8 . Further, in order for the REST API in my Quarkus application to be easily consumable from external, JavaScript-based services and applications, I needed to enable CORS. A word of warning is important here with regards to the CORS “origin”, that should be tailored to your production use case (as ); if you decide to build on this example, you might want to consider for your allow-list to be specific to the expected origin of your clients (in my case Swagger UI from OpenShift and Excel ScriptLabs, but you might want to extend to the servers of your Office Add-In, etc): quarkus.http.cors=true # note: check settings for PROD: quarkus.http.cors.origins=/.*\\.azureedge\\.net/,/.*\\.openshiftapps\\.com/ quarkus.swagger-ui.always-include=true quarkus.kubernetes.deployment-target=knative quarkus.container-image.registry=quay.io quarkus.container-image.group=mmortari quarkus.container-image.builder=jib In addition to the CORS configuration, it’s pretty easy to influence the behavior of the final resulting Quarkus application, specifically: * I want the Swagger UI to be included in the deployed artifact * it will be a Knative Service, so to allow the serveless use-case, including auto-scale to zero * I find easier to publish my container images on Quay.io, to be picked up by my OpenShift instance * to build the container image, I typically use JIB These configuration steps are similar to what described in , showcasing how it’s really easy to build a Serverless application with Drools and Quarkus! Be sure to if you missed it.😉 EXCEL INTEGRATION Here comes the very unusual part, at least for me, where I wanted to apply some of the techniques from the book and then explore even further.🙂  First, I collected all the data from my IoT devices; personally I own a couple of smart devices from , as I appreciate they allow you to easily export an archive of your data, in CSV format: perfect for Excel! Similarly, you might consider expanding on this example by directly interacting instead . The archive exports a ZIP of a collection of CSV files; for my challenge I indeed decided to focus on bpm and weight measurements, which are actually in 2 separate files. To combine this data into a single table I’ve used a Power Query, one of the capabilities presented in the book, in order to connect to the CSV files as data sources and merge them seamlessly. The merge result is something similar to: Merging 2 CSV files in Excel Then, I have defined a custom function in Excel; you can find more information about this capability on as it is one of the most powerful mechanisms available to extend Excel with custom behavior. I should highlight that in the book, you will find many, many other mechanisms to perform an invocation from your Excel sheets to a remote Drools application running on OpenShift; personally, I opted to develop a custom function in order to try something new but also sophisticated, which could be bundled later as a fully-fledged Office Add-In; but the book indeed guides you through many more (and often easier) mechanisms! One of the reasons I loved that read so much, is that it offered a wide portfolio of options to choose from when it comes to integrating Excel with Drools. My final custom Excel function looks like this: /** @CustomFunction */ function advices(isodate: string, bpm: number, weight: number, weight_m3: number, weight_m5: number): Promise&lt;string&gt; { return new Promise(function (resolve, reject) { const baseUrl = "https://(...).openshiftapps.com"; const payload = JSON.stringify( { "ts": isodate, "bpm": bpm &gt; 0 ? bpm : null, "weight": weight, "weight_m3": weight_m3, "weight_m5": weight_m5 } ); fetch(baseUrl + "/advices", { method: "POST", body: payload, headers: { "Content-type": "application/json; charset=UTF-8" } }) .then((response) =&gt; response.json()) .then((json) =&gt; resolve(json)) .catch((error) =&gt; reject("unable to connect to Drools")) }); } …and it works like a charm! The custom function is invoked by a very simple Excel formula, as one would easily expect: Excel custom function to invoke Drools! It is also to be noted, again as expected, that when the formulas has been computed for the entire worksheet, the backend Knative service will automatically scale back to zero: a Knative based serverless backend serving the Excel formulas! This is super helpful only to consume computing resources when needed, in this case when some Excel worksheet needs to (re-)calculate its formulas!  As the final and most important result, we can appreciate the rules processing the data and producing the advice in our Excel file, as defined in the DRL. I believe combining Excel custom and extended behaviors with a serverless backend is truly a powerful combination! Thankfully integrating Quarkus and Drools and deploying our app on OpenShift with Knative is super easy as we’ve seen in this post. I hope this atypical blog post tickles your curiosity on how to integrate Excel or similarly other spreadsheet platforms; if you are interested to know more, I warmly invite you to check out this book… BOOK REVIEW: BUSINESS RULE ENGINES AND AI FOR EXCEL POWER USERS Title: Business Rule Engines and AI for Excel Power Users: Capture and scale your business knowledge into the cloud – with Microsoft 365, Decision Models, and AI tools from IBM and Red Hat Author: Paul Browne ISBN: 9781804619544 (ISBN10: 180461954X) I believe this book is an excellent guide for both software developers and business analysts seeking to scale the automation of their business knowledge into the cloud. It provides an in-depth analysis of how decision models and semantic rules can be combined with other AI models, to solve some of the inherent limitations of Excel –which is an omnipresent tool in every business and industry sector. The book introduces readers to industry-standard open source Drools rule engine and Kogito, and how these can be linked with many of Microsoft’s tools. Paul presents very easy-to-follow examples to teach readers how to author sophisticated decision models, how to develop decision services in order to solve current business challenges using AI (both ML and symbolic AI), and how to combine rules with workflows to deploy a cloud-based solution. The book also covers advanced modeling using the Decision Model and Notation (DMN) open standard and related testing tools. As a reader of this blog, I assume you are already familiar with some of the KIE projects, so you might be tempted to jump straight into reading from Chapter 6 onwards; but my recommendation would be to make sure to revisit the initial chapters nevertheless, especially Chapters 1-2, since they will equip you with important considerations when evaluating the adoption of the powerful techniques presented in this book in your organization. It is also to be noted that while specific to Microsoft tools, the techniques presented in this book (and this inspired blog post) can very likely be analogously applied using other software provider platforms and other hyperscalers! CONCLUSION I hope this blog post intrigued you to check out this new book and to explore more integration opportunities of Drools with other platforms and tools! How are you planning to integrate Drools for your next use-case? Let us know in the comments section below! The post appeared first on .</content><dc:creator>Matteo Mortari</dc:creator></entry><entry><title>New C features in GCC 13</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/04/new-c-features-gcc-13" /><author><name>Marek Polacek</name></author><id>bbc67a05-6b6c-4d0c-a0eb-392635912965</id><updated>2023-05-04T07:00:00Z</updated><published>2023-05-04T07:00:00Z</published><summary type="html">&lt;p&gt;The latest major version of the&lt;a href="https://gcc.gnu.org/"&gt; GNU Compiler Collection&lt;/a&gt; (GCC), 13.1, was released in April 2023. Like every major GCC release, this version will bring many&lt;a href="https://gcc.gnu.org/gcc-13/changes.html"&gt; additions, improvements, bug fixes, and new features&lt;/a&gt;. GCC 13 is already the system compiler in&lt;a href="https://fedoraproject.org/wiki/Changes/GNUToolchainF38"&gt; Fedora 38&lt;/a&gt;. &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) users will get GCC 13 in the Red Hat GCC Toolset (RHEL 8 and RHEL 9). It's also possible to try GCC 13 on &lt;a href="https://godbolt.org/"&gt;godbolt.org&lt;/a&gt; and similar pages.&lt;/p&gt; &lt;p&gt;This article describes new features implemented in the C front end; it does not discuss developments in &lt;a href="https://developers.redhat.com/topics/c"&gt;the C language&lt;/a&gt; itself. It also doesn’t cover recent changes in the C library itself. If you’re interested in the C++ language and what's supported in recent GCC releases, check out &lt;a href="https://developers.redhat.com/blog/2020/09/24/new-c-features-in-gcc-10"&gt;New C++ features in GCC 10&lt;/a&gt; and &lt;a href="https://developers.redhat.com/articles/2022/04/25/new-c-features-gcc-12"&gt;New C++ features in GCC 12&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The default C dialect in GCC 13 is &lt;code&gt;-std=gnu17&lt;/code&gt;. You can use the &lt;code&gt;-std=c2x&lt;/code&gt; or &lt;code&gt;-std=gnu2x&lt;/code&gt; command-line options to enable C2X features. We use C2X to refer to the next major C standard version; it is expected to become C23.&lt;/p&gt; &lt;h2&gt;C2X features&lt;/h2&gt; &lt;p&gt;GCC 13 has implemented a host of C2X proposals. This section describes the most interesting ones.&lt;/p&gt; &lt;h3&gt;nullptr&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;nullptr&lt;/code&gt; constant first appeared in C++11, described in proposal &lt;a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2431.pdf"&gt;N2431&lt;/a&gt; from 2007. Its purpose was to alleviate the problems with the definition of &lt;code&gt;NULL&lt;/code&gt;, which can be defined in a variety of ways: &lt;code&gt;(void *)0&lt;/code&gt; (a pointer constant), &lt;code&gt;0&lt;/code&gt; (an integer), and so on. This posed problems for overload resolution, generic programming, etc. While C doesn’t have function overloading, the protean definition of &lt;code&gt;NULL&lt;/code&gt; still causes headaches. Consider the interaction of &lt;code&gt;_Generic&lt;/code&gt; with &lt;code&gt;NULL&lt;/code&gt;: it’s not clear which function will be called because it depends on the definition of &lt;code&gt;NULL&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; _Generic (NULL, void *: handle_ptr (), int: crash (), default: nop ()); &lt;/pre&gt; &lt;p&gt;Unfortunately, there are less contrived problems in practice. For instance, issues occur with conditional operators or when passing &lt;code&gt;NULL&lt;/code&gt; to a variadic function (taking &lt;code&gt;...&lt;/code&gt;): in such a case, applying &lt;code&gt;va_arg&lt;/code&gt; to the null argument may crash the program if an unexpected definition of &lt;code&gt;NULL&lt;/code&gt; is encountered. GCC 13 implements &lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3042.htm"&gt;N3042&lt;/a&gt;, which brings &lt;code&gt;nullptr&lt;/code&gt; to C. Its type is &lt;code&gt;nullptr_t&lt;/code&gt; and is defined in &lt;code&gt;&lt;stddef.h&gt;&lt;/code&gt;. In C2X, the following assert therefore passes:&lt;/p&gt; &lt;pre&gt; static_assert (_Generic (nullptr, nullptr_t: 1, void *: 2, default: 0) == 1, "nullptr_t was selected"); &lt;/pre&gt; &lt;h3&gt;Enhanced enumerations&lt;/h3&gt; &lt;p&gt;Enhanced enumerations is another feature that first appeared in C++11 via &lt;a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2347.pdf"&gt;N2347&lt;/a&gt;. In C, the underlying type of an enum was not specified in the standard. In practice, the type would be determined based on the values of the enumerators. Typically, the type would be &lt;code&gt;unsigned int&lt;/code&gt;, or, if any of the values is negative, &lt;code&gt;int&lt;/code&gt;. In any case, the selected type must be capable of holding all of the values of the enum. Given this lacuna in the specification, enums have portability issues. To close this gap, C adopted &lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2963.htm"&gt;N2963&lt;/a&gt;, adopting the C++ syntax:&lt;/p&gt; &lt;pre&gt; enum E : long long { R, G, B } e; static_assert (_Generic (e, long long: 1, default: 0) == 1, "E type");&lt;/pre&gt; &lt;p&gt;It seems worth mentioning, however, that specifying the wrong underlying type may lead to subtle problems.  Consider the following:&lt;/p&gt; &lt;pre&gt; enum F : int { A = 0x8000 } f; &lt;/pre&gt; &lt;p&gt;On most platforms, this code will work as expected. The precision of &lt;code&gt;int&lt;/code&gt; isn’t guaranteed to be at least 32 bits, however; it can validly be 16 bits, in which case the previous example will not compile. Thus a better variant would be to use one of the types defined in &lt;code&gt;&lt;stdint.h&gt;&lt;/code&gt;, for example:&lt;/p&gt; &lt;pre&gt; enum F : int_least32_t { A = 0x8000 } f; &lt;/pre&gt; &lt;h3&gt;(...) function prototypes&lt;/h3&gt; &lt;p&gt;C, prior to C2X, required that a variable-argument function has a named argument before the ellipsis (&lt;code&gt;...&lt;/code&gt;). This requirement was the result of historical baggage and is no longer necessary, so &lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2975.pdf"&gt;N2975&lt;/a&gt; did away with the requirement. (C++ has always allowed &lt;code&gt;foo(...)&lt;/code&gt;.)&lt;/p&gt; &lt;pre&gt; void f(int, ...); // OK void g(...); // OK in C2X &lt;/pre&gt; &lt;p&gt;Note, however, that &lt;code&gt;fn(...)&lt;/code&gt; is &lt;em&gt;not&lt;/em&gt; an unprototyped function, so it is possible to use the &lt;code&gt;va_start&lt;/code&gt; and &lt;code&gt;va_arg&lt;/code&gt; mechanism to access its arguments. An unprototyped function has the form &lt;code&gt;void u();&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Such functions were removed in C2X (see below).&lt;/p&gt; &lt;h3&gt;Type inference with auto&lt;/h3&gt; &lt;p&gt;Type deduction is another feature that first appeared in C++11 via &lt;a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n1984.pdf"&gt;N1984&lt;/a&gt;. It is a convenient feature that allows the programmer to use the placeholder &lt;code&gt;auto&lt;/code&gt; as the type in a declaration. The compiler will then deduce the variable’s type from the initializer:&lt;/p&gt; &lt;pre&gt; auto i = 42; &lt;/pre&gt; &lt;p&gt;It is, however, more than just a convenience feature to save typing a few more characters. Consider:&lt;/p&gt; &lt;pre&gt; auto x = foo (y); &lt;/pre&gt; &lt;p&gt;Here, the type of &lt;code&gt;foo (y)&lt;/code&gt; may depend on &lt;code&gt;y&lt;/code&gt; (&lt;code&gt;foo&lt;/code&gt; could be a macro using &lt;code&gt;_Generic&lt;/code&gt;), so changing &lt;code&gt;y&lt;/code&gt; implies changing the type of &lt;code&gt;x&lt;/code&gt;. Using &lt;code&gt;auto&lt;/code&gt; in the example above means that the programmer doesn’t have to change the rest of the codebase when the type of &lt;code&gt;y&lt;/code&gt; is updated. GCC has offered &lt;code&gt;__auto_type&lt;/code&gt; since &lt;a href="https://gcc.gnu.org/gcc-4.9/changes.html"&gt;GCC 4.9&lt;/a&gt;, whose semantics is fairly close to C2X &lt;code&gt;auto&lt;/code&gt;, though not exactly the same, and appears to have been used mostly in standard headers. Unlike C++, &lt;code&gt;auto&lt;/code&gt; must be used plain: it cannot be combined with &lt;code&gt;*&lt;/code&gt; or &lt;code&gt;[]&lt;/code&gt; and similar. Moreover, &lt;code&gt;auto&lt;/code&gt; also doesn’t support braces around the initializer. The &lt;code&gt;auto&lt;/code&gt; feature is only enabled in C2X mode. In older modes, &lt;code&gt;auto&lt;/code&gt; is a redundant storage class specifier which can only be used at block scope.&lt;/p&gt; &lt;h3&gt;The constexpr specifier&lt;/h3&gt; &lt;p&gt;Yet another feature that first appeared in C++ is the &lt;code&gt;constexpr&lt;/code&gt; specifier (see for instance &lt;a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2235.pdf"&gt;N2235&lt;/a&gt;, though C++ &lt;code&gt;constexpr&lt;/code&gt; has been greatly expanded since). C &lt;code&gt;constexpr&lt;/code&gt; was introduced in &lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3018.htm"&gt;N3018&lt;/a&gt;, with much more limited functionality. Declaring a variable as &lt;code&gt;constexpr&lt;/code&gt; guarantees that the variable can be used in various constant-expression contexts. C requires that objects with static storage duration are initialized with constant expressions. It follows that &lt;code&gt;constexpr&lt;/code&gt; variables can be used to initialize objects with static storage duration. Another great advantage of &lt;code&gt;constexpr&lt;/code&gt; is that various semantic constraints are checked at compile time. Let’s demonstrate both points with an example (note that you must specify &lt;code&gt;-std=c2x&lt;/code&gt; or &lt;code&gt;-std=gnu2x&lt;/code&gt; to be able to use &lt;code&gt;constexpr&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt; constexpr int i = 12; static_assert (i == 12); struct X { int bf : i; }; struct S { long l; }; constexpr struct S s = { 1L }; static_assert (s.l == 1L); constexpr unsigned char q = 0xff + i; // initializer not representable in type of object&lt;/pre&gt; &lt;h3&gt;Storage-class specifiers in compound literals&lt;/h3&gt; &lt;p&gt;A compound literal is a way to create unnamed objects that typically have automatic storage duration. Because they are lvalues, it is permitted to take their address:&lt;/p&gt; &lt;pre&gt; int *p = (int []){2, 4}; // p points to the first element of an array of two ints const int *q = &amp;(const int){1}; &lt;/pre&gt; &lt;p&gt;Paper &lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3038.htm"&gt;N3038&lt;/a&gt; allows using certain storage-class specifiers (things like &lt;code&gt;constexpr&lt;/code&gt;, &lt;code&gt;static&lt;/code&gt;, &lt;code&gt;thread_local&lt;/code&gt;) in compound literals in C2X mode. This is useful to change the lifetime of the compound literal, or to make it a &lt;em&gt;compound literal constant&lt;/em&gt; with the &lt;code&gt;constexpr&lt;/code&gt; keyword:&lt;/p&gt; &lt;pre&gt; struct S { int i; }; void f (void) { static struct S s = (constexpr struct S){ 42 }; } int * g (void) { return &amp;(static int){ 42 }; } &lt;/pre&gt; &lt;p&gt;Note that even though &lt;code&gt;typedef&lt;/code&gt;, &lt;code&gt;extern&lt;/code&gt;, and &lt;code&gt;auto&lt;/code&gt; are storage-class specifiers, they are not allowed in compound literals.&lt;/p&gt; &lt;h3&gt;C2X typeof&lt;/h3&gt; &lt;p&gt;C2X standardized &lt;code&gt;typeof&lt;/code&gt;, a feature that has been supported as a GNU extension for many years which allows the programmer to get the type of an expression as described &lt;a href="https://gcc.gnu.org/onlinedocs/gcc/Typeof.html"&gt;here&lt;/a&gt;. Along with &lt;code&gt;typeof&lt;/code&gt;, C2X also adds &lt;code&gt;typeof_unqual&lt;/code&gt;, which additionally removes all qualifiers and &lt;code&gt;_Atomic&lt;/code&gt; from the resulting type:&lt;/p&gt; &lt;pre&gt; int i; volatile int vi; extern typeof (vi) vi; // OK, no conflict extern typeof_unqual (vi) i; // OK, no conflict &lt;/pre&gt; &lt;p&gt;A minor difference between the GNU version and the standard version is the treatment of the noreturn property of a function: the GNU variant of &lt;code&gt;typeof&lt;/code&gt; takes noreturn as part of the type of a pointer to function, but the standard version does not.&lt;/p&gt; &lt;p&gt;Note that C++11 standardized a similar feature under the name &lt;code&gt;decltype&lt;/code&gt;, so sadly we wound up with two names for a nearly identical feature.&lt;/p&gt; &lt;h3&gt;New keywords&lt;/h3&gt; &lt;p&gt;This proposal harmonizes C and C++ further by making &lt;code&gt;alignas&lt;/code&gt;, &lt;code&gt;alignof&lt;/code&gt;, &lt;code&gt;bool&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;, &lt;code&gt;static_assert&lt;/code&gt;, &lt;code&gt;thread_local&lt;/code&gt;, and &lt;code&gt;true&lt;/code&gt; ordinary keywords in C2X mode. Therefore this translation unit will compile OK in C2X mode:&lt;/p&gt; &lt;pre&gt; static_assert (true, ""); &lt;/pre&gt; &lt;p&gt;This change can break existing code, for example&lt;/p&gt; &lt;pre&gt; int alignof = 42; &lt;/pre&gt; &lt;p&gt;will not compile in C2X mode.&lt;/p&gt; &lt;h3&gt;The noreturn attribute&lt;/h3&gt; &lt;p&gt;A further compatibility tweak to bring C and C++ closer together. C11 added the &lt;code&gt;_Noreturn&lt;/code&gt; function specifier to signal to the compiler that a function never returns to its caller, but &lt;code&gt;_Noreturn&lt;/code&gt; works in C only, so C2X &lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2764.pdf"&gt;N2764&lt;/a&gt; added a standard &lt;code&gt;[[noreturn]]&lt;/code&gt; attribute while simultaneously marking &lt;code&gt;_Noreturn&lt;/code&gt; as obsolescent.&lt;/p&gt; &lt;pre&gt; [[noreturn]] void exit (int); &lt;/pre&gt; &lt;h3&gt;Empty initializer braces&lt;/h3&gt; &lt;p&gt;C2X standardized empty initializer braces (&lt;code&gt;{}&lt;/code&gt;) and GCC 13 implements this proposal. Some cases were already supported as a GNU extension (e.g., initializing an array or a structure), but newly it’s possible to use &lt;code&gt;{}&lt;/code&gt; to initialize a scalar variable or a variable-length array as well:&lt;/p&gt; &lt;pre&gt; int i = {}; int arr[10] = {}; struct S { int i; }; struct S s = {}; void g (void) { int n = 10; int vla[n] = {}; } &lt;/pre&gt; &lt;h3&gt;unreachable macro&lt;/h3&gt; &lt;p&gt;C2X brings the &lt;code&gt;unreachable()&lt;/code&gt; macro, defined in &lt;code&gt;&lt;stddef.h&gt;&lt;/code&gt;, which is a convenient shorthand for the GCC built-in function &lt;code&gt;__builtin_unreachable()&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; #include &lt;stddef.h&gt; int foo (int x) { if (x &lt; 0) unreachable (); return x &amp; 1; } &lt;/pre&gt; &lt;h3&gt;Unprototyped functions removed&lt;/h3&gt; &lt;p&gt;Unprototyped functions in C were of the form &lt;code&gt;int foo()&lt;/code&gt;, which is a function &lt;code&gt;foo&lt;/code&gt; that returns an integer which takes an unspecified number of arguments of unspecified types. This is very dangerous because the compiler can’t perform any checking when such a function is used.&lt;/p&gt; &lt;p&gt;In C2X, &lt;code&gt;int foo()&lt;/code&gt; is equivalent to &lt;code&gt;int foo(void)&lt;/code&gt;, which is a function &lt;code&gt;foo&lt;/code&gt; that returns an integer and takes no arguments.&lt;/p&gt; &lt;h2&gt;New warnings&lt;/h2&gt; &lt;p&gt;The C front end has gained some new warnings in GCC 13. For instance, &lt;a href="https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wxor-used-as-pow"&gt;-Wxor-used-as-pow&lt;/a&gt;, which was described in the C++ part of the GCC 13 blog post. There’s a new warning specific for the C front end.&lt;/p&gt; &lt;h3&gt;-Wenum-int-mismatch&lt;/h3&gt; &lt;p&gt;In C, an enumerated type is compatible with char, a signed integer type, or an unsigned integer type, so the following code compiles if the underlying type of &lt;code&gt;enum E&lt;/code&gt; is &lt;code&gt;int&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; enum E { l = -1, z = 0, g = 1 }; int foo(void); enum E foo(void) { return z; } &lt;/pre&gt; &lt;p&gt;However, as I previously noted, the choice of the underlying type of the enum is implementation-defined. Since the code above is likely a mistake and constitutes a portability problem (the code will not compile if a different type than &lt;code&gt;int&lt;/code&gt; is chosen to be the underlying type), GCC 13 implements a new warning which warns about enum/integer type mismatches. For the code above the warning looks like the following:&lt;/p&gt; &lt;pre&gt; q.c:5:10: warning: conflicting types for ‘foo’ due to enum/integer mismatch; have ‘enum E(void)’ [-Wenum-int-mismatch] 5 | enum E foo(void) { return z; } | ^~~ q.c:4:7: note: previous declaration of ‘foo’ with type ‘int(void)’ 4 | int foo(void); | ^~~ &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;GCC 13 implements many C2X proposals. These proposals align the C and C++ languages a little bit closer to each other by entwining certain features, and make programming in C easier and more secure.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/04/new-c-features-gcc-13" title="New C features in GCC 13"&gt;New C features in GCC 13&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Marek Polacek</dc:creator><dc:date>2023-05-04T07:00:00Z</dc:date></entry><entry><title>Quarkus 3.0.2.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-0-2-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-0-2-final-released/</id><updated>2023-05-04T00:00:00Z</updated><published>2023-05-04T00:00:00Z</published><summary type="html">We released Quarkus 3.0.2.Final, the first maintenance release of our 3.0 release train (as our first public release for 3.0 was 3.0.1.Final). As usual, it contains bugfixes and documentation improvements. It should be a safe upgrade for anyone already using 3.0. If you are not already using 3.0, please refer...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-05-04T00:00:00Z</dc:date></entry><entry><title>How to debug C++ lambda expressions with GDB</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/03/how-debug-c-lambda-expressions-gdb" /><author><name>Kevin Buettner</name></author><id>f017049c-3b61-4400-b158-a767e759946e</id><updated>2023-05-03T07:00:00Z</updated><published>2023-05-03T07:00:00Z</published><summary type="html">&lt;p&gt;Modern versions of the &lt;a href="https://developers.redhat.com/topics/c"&gt;C++&lt;/a&gt; programming language have a feature known as &lt;a href="https://en.cppreference.com/w/cpp/language/lambda"&gt;lambda expressions&lt;/a&gt;. This article shows how you can debug lambda expressions using GDB, the GNU Project Debugger. Even if you're not interested in debugging lambdas, the techniques presented here are useful for many other debugging situations.&lt;/p&gt; &lt;h2&gt;What is a lambda expression?&lt;/h2&gt; &lt;p&gt;A lambda expression provides the C++ programmer with a way to create an anonymous or unnamed function. Lambda expressions are often used in situations where a callback function is desired. Using a lambda expression often makes writing a callback function significantly easier since various pieces of state that are known in the function from which the callback is passed don't need to be packaged up into a data structure which the callback function will later access. This is due to the fact that a C++ lambda expression provides a way to capture in-scope variables which may be later used when the lambda expression is executed.&lt;/p&gt; &lt;p&gt;The example below shows some simple captures in a few of its lambda expressions.&lt;/p&gt; &lt;h2&gt;Example program&lt;/h2&gt; &lt;p&gt;I'll use the example program below to demonstrate some debugging techniques in addition to showing some of the challenges that might be encountered when debugging lambda expressions. I've included line numbers as comments for some of the lines at which breakpoints might be placed. I've named this program &lt;code&gt;lambda.cc&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#include &lt;stdio.h&gt; #include &lt;functional&gt; int successor (int i) { return i + 1; } int apply0 (int (*fn)(int), int arg) { return fn (arg); } int apply1 (std::function&lt;int(int)&gt; fn, int arg) { return fn (arg); } std::function&lt;int(int)&gt; make_function(int&amp; x) { return [&amp;] (int i) { return i + x; }; /* Line 17 */ } int main (int argc, char **argv) { int n = 7, m = -28; printf ("Answer 1 is %d\n", apply0 (successor, 3)); /* Line 24 */ printf ("Answer 2 is %d\n", apply1 (successor, 4)); /* Line 25 */ printf ("Answer 3 is %d\n", apply0 ([] (int i) { return i + 1; }, 1)); /* Line 28 */ printf ("Answer 4 is %d\n", apply1 ([] (int i) { return i + 1; }, 2)); /* Line 30 */ printf ("Answer 5 is %d\n", apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */ auto lf2 = make_function (n); /* Line 35 */ printf ("Answer 6 is %d\n", apply1 (lf2, 1)); /* Line 36 */ auto lf3 = make_function (m); printf ("Answer 7 is %d\n", apply1 (lf3, -14)); /* Line 39 */ } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Lines 17, 28, 30, and 33 all contain lambda expressions. The lambda expression on line 33 is:&lt;/p&gt; &lt;p&gt;&lt;code&gt;[n] (int i) { return i + n; }&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Lambda expressions start with a left square bracket. In this particular lambda expression the left square bracket is followed by &lt;code&gt;n&lt;/code&gt;, which is a variable that is being &lt;em&gt;captured&lt;/em&gt; for use in the body of the lambda. Note that &lt;code&gt;n&lt;/code&gt; is a local variable in the function &lt;code&gt;main&lt;/code&gt;. Parentheses enclose a list of formal parameters to the anonymous function being defined; in this case there is just one parameter named &lt;code&gt;i&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Finally, the body of the lambda expression is placed between curly braces, just like a normal function. The body consists of zero or more executable statements. In this case, there is just one executable statement, a return statement. But do note that the expression to return refers to both the captured variable &lt;code&gt;n&lt;/code&gt; and the parameter &lt;code&gt;i&lt;/code&gt;. Note that there is no function name—that's what makes it anonymous. Lambda expressions are sometimes assigned to variables; this is done indirectly on lines 35 and 38. There are other optional syntactic components as well as many nuances of lambda expressions which are not described here.&lt;/p&gt; &lt;h2&gt;Building and debugging the example program&lt;/h2&gt; &lt;p&gt;You can use the GNU C++ compiler to compile and link the example program by using this command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;g++ -Wall -g -o lambda lambda.cc&lt;/code&gt;&lt;/p&gt; &lt;p&gt;A similar command can be used to build an executable using the LLVM C++ compiler (clang++):&lt;/p&gt; &lt;p&gt;&lt;code&gt;clang++ -Wall -g -o lambda-clang lambda.cc &lt;/code&gt;&lt;/p&gt; &lt;p&gt;GDB interactions shown below were performed using GDB 13.1 on a Fedora 37 machine. Except where noted, the example program was compiled using GCC (g++) 12.2.1. When I did use LLVM (clang++) to check compatibility, I used Clang version 15.0.7.&lt;/p&gt; &lt;p&gt;We can begin debugging the example program using GDB as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ gdb -q lambda Reading symbols from lambda... (gdb) start Temporary breakpoint 1 at 0x401275: file lambda.cc, line 22. Starting program: /home/kev/examples/lambda This GDB supports auto-downloading debuginfo from the following URLs: &lt;https://debuginfod.fedoraproject.org/&gt; Enable debuginfod for this session? (y or [n]) y Debuginfod has been enabled. To make this setting permanent, add 'set debuginfod enabled on' to .gdbinit. [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1". Temporary breakpoint 1, main (argc=1, argv=0x7fffffffdd88) at lambda.cc:22 22 int n = 7, m = -28; (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This example shows how GDB is invoked on the executable named &lt;code&gt;lambda&lt;/code&gt;. The &lt;code&gt;-q&lt;/code&gt; switch causes GDB to not display copyright, warranty and information about how to obtain documentation and help. Once in GDB, the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Starting.html#index-start"&gt;&lt;code&gt;start&lt;/code&gt; command&lt;/a&gt; is used to run to the first executable line in the &lt;code&gt;main&lt;/code&gt; function.&lt;/p&gt; &lt;p&gt;Note, too, that I answered &lt;code&gt;y&lt;/code&gt; to enable &lt;a href="https://developers.redhat.com/blog/2019/10/14/introducing-debuginfod-the-elfutils-debuginfo-server"&gt;debuginfod&lt;/a&gt; for the session. When debuginfod is enabled, debugging information associated with libraries used by the program may be downloaded for use by GDB.&lt;/p&gt; &lt;p&gt;In the following examples, I won't show (again) the initial step of compiling the program nor the initial steps of debugging the program with GDB. That said, if you want to follow along, I've organized the examples (up to the section &lt;a href="#Debugging an LLVM compiled program"&gt;Debugging an LLVM compiled program&lt;/a&gt;) so that you won't need to restart GDB—you should be able to obtain similar output by simply typing the commands shown here to GDB.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; if the version of GDB or version of the compiler used to build the example program differ significantly form the versions noted earlier, it's possible that you'll see output different from what I show here.&lt;/p&gt; &lt;h2&gt;Debugging apply0 and successor&lt;/h2&gt; &lt;p&gt;Before looking at debugging a lambda expression, let's first look at using GDB to step into the &lt;code&gt;apply0&lt;/code&gt; call at line 24. Note that &lt;code&gt;successor&lt;/code&gt; is passed to &lt;code&gt;apply0&lt;/code&gt;; the &lt;code&gt;successor&lt;/code&gt; function adds one to its &lt;code&gt;int&lt;/code&gt; argument and returns this value.  &lt;code&gt;apply0&lt;/code&gt; takes two arguments, the first of which is a pointer to a function with the second being the value to pass to that function.  It simply calls the function with the argument and returns the result. Stepping into &lt;code&gt;apply0&lt;/code&gt; and then &lt;code&gt;successor&lt;/code&gt; is straightforward:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) until 24 main (argc=1, argv=0x7fffffffdd88) at lambda1.cc:24 24 printf ("Answer 0.0 is %d\n", apply0 (successor, 3)); (gdb) step apply0 (fn=0x401156 &lt;successor(int)&gt;, arg=3) at lambda1.cc:8 8 return fn (arg); (gdb) step successor (i=3) at lambda1.cc:4 4 int successor (int i) { return i + 1; } (gdb) print i $1 = 3 (gdb) backtrace #0 successor (i=3) at lambda1.cc:4 #1 0x000000000040117f in apply0 (fn=0x401156 &lt;successor(int)&gt;, arg=3) at lambda1.cc:8 #2 0x0000000000401305 in main (argc=1, argv=0x7fffffffdd88) at lambda1.cc:24 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The above example shows GDB's &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Continuing-and-Stepping.html#index-until"&gt;&lt;code&gt;until&lt;/code&gt; command&lt;/a&gt;, which, in this case, can be used to advance program execution to the specified line, in this case line 24. (Note: The &lt;code&gt;until&lt;/code&gt; command is used for other purposes too; don't expect it to always advance to the specified line number.)&lt;/p&gt; &lt;p&gt;Next, the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Continuing-and-Stepping.html#index-step"&gt;&lt;code&gt;step&lt;/code&gt; command&lt;/a&gt; is used twice, once to step into &lt;code&gt;apply0&lt;/code&gt; and then again into &lt;code&gt;successor&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;After that, a &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Data.html#index-print"&gt;&lt;code&gt;print&lt;/code&gt; command&lt;/a&gt; is used to show the value of the argument &lt;code&gt;i&lt;/code&gt; which is in the &lt;code&gt;successor&lt;/code&gt; function.&lt;/p&gt; &lt;p&gt;Finally, the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Backtrace.html#index-backtrace"&gt;&lt;code&gt;backtrace&lt;/code&gt; command&lt;/a&gt; is used to show the stack trace.&lt;/p&gt; &lt;p&gt;These commands are straightforward and should be unsurprising to anyone accustomed to using GDB. Ideally, we'd like the debugging of lambda expressions to be just as straightforward, though we'll soon see that this is not the case.&lt;/p&gt; &lt;h2&gt;Function objects: Debugging apply1 and successor&lt;/h2&gt; &lt;p&gt;The first gotcha occurs when using a function object. To demonstrate this, I've defined &lt;code&gt;apply1&lt;/code&gt; to be very similar to &lt;code&gt;apply0&lt;/code&gt;, with the only difference being that instead of taking a function pointer as the first argument (as is done in &lt;code&gt;apply0&lt;/code&gt;), the first argument of &lt;code&gt;apply1&lt;/code&gt; is a function object which takes an int argument and returns an int. As a reminder, this is what &lt;code&gt;apply1&lt;/code&gt; looks like:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;int apply1 (std::function&lt;int(int)&gt; fn, int arg) { return fn (arg); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In GDB, we'll first advance to line 25 by first using the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Set-Breaks.html#index-tbreak"&gt;&lt;code&gt;tbreak&lt;/code&gt; command&lt;/a&gt; to place a temporary breakpoint, after which we use the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Continuing-and-Stepping.html#index-c-_0028continue_0029"&gt;&lt;code&gt;continue&lt;/code&gt; command&lt;/a&gt; to advance to that breakpoint:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) tbreak 25 Temporary breakpoint 2 at 0x4012a9: file lambda.cc, line 25. (gdb) continue Continuing. Answer 1 is 4 Temporary breakpoint 2, main (argc=1, argv=0x7fffffffdd98) at lambda.cc:25 25 printf ("Answer 2 is %d\n", apply1 (successor, 4)); /* Line 25 */ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before attempting to step into &lt;code&gt;apply1&lt;/code&gt;, let's create a checkpoint to which we'll come back later:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) checkpoint checkpoint 1: fork returned pid 4178814. &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; GDB's &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Checkpoint_002fRestart.html#index-checkpoint-1"&gt;&lt;code&gt;checkpoint&lt;/code&gt; command&lt;/a&gt; has several limitations: it doesn't work for multi-threaded programs and it also only works on GNU/Linux. But when it can be used, it's very handy for situations where you might wish to return to an earlier program state.&lt;/p&gt; &lt;p&gt;Now, let's see what happens when we try to step into &lt;code&gt;apply1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) step std::function&lt;int (int)&gt;::function&lt;int (&amp;)(int), void&gt;(int (&amp;)(int)) ( this=0x7fffffffdb90, __f=@0x401156: {int (int)} 0x401156 &lt;successor(int)&gt;) at /usr/include/c++/12/bits/std_function.h:437 437 : _Function_base() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What has happened here is that we've stepped into code which is constructing the function object, which is not what we wanted—we wanted to step into &lt;code&gt;apply1&lt;/code&gt;. To get past this, we can use the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Continuing-and-Stepping.html#index-fin-_0028finish_0029"&gt;&lt;code&gt;finish&lt;/code&gt; command&lt;/a&gt; (which returns us to &lt;code&gt;main&lt;/code&gt; at line 25) and then &lt;code&gt;step&lt;/code&gt; again, which will get us into &lt;code&gt;apply1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) finish Run till exit from #0 std::function&lt;int (int)&gt;::function&lt;int (&amp;)(int), void&gt;(int (&amp;)(int)) (this=0x7fffffffdb90, __f=@0x401156: {int (int)} 0x401156 &lt;successor(int)&gt;) at /usr/include/c++/12/bits/std_function.h:437 0x00000000004012bd in main (argc=1, argv=0x7fffffffdd88) at lambda.cc:25 25 printf ("Answer 2 is %d\n", apply1 (successor, 4)); /* Line 25 */ (gdb) step apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 13 return fn (arg);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A more complicated call might require the GDB user to issue multiple &lt;code&gt;finish&lt;/code&gt; / &lt;code&gt;step&lt;/code&gt; commands in order to end up in the desired function.&lt;/p&gt; &lt;h2&gt;Using GDB's skip command to avoid stepping into function object constructors&lt;/h2&gt; &lt;p&gt;Instead of using &lt;code&gt;finish&lt;/code&gt; and then &lt;code&gt;step&lt;/code&gt; as shown above, let's look at a way that GDB can more directly step into apply1. First, we'll return to the checkpoint that we created earlier—this is done by using the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Checkpoint_002fRestart.html#index-restart-checkpoint_002did"&gt;&lt;code&gt;restart&lt;/code&gt; command&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) restart 1 Switching to Thread 0x7ffff7a89400 (LWP 4178814) #0 main (argc=1, argv=0x7fffffffdd98) at lambda.cc:25 25 printf ("Answer 2 is %d\n", apply1 (successor, 4)); /* Line 25 */ (gdb)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now we'll use GDB's &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Skipping-Over-Functions-and-Files.html#index-skip"&gt;&lt;code&gt;skip&lt;/code&gt; command&lt;/a&gt;.  The &lt;code&gt;skip&lt;/code&gt; command shown here will cause GDB to skip, while stepping, any calls to any method in the std::function class, which includes the constructor that we ran into earlier. This particular &lt;code&gt;skip&lt;/code&gt; command uses the &lt;code&gt;-rfunction&lt;/code&gt; option with the regular expression &lt;code&gt;^std::function.*&lt;/code&gt;. The &lt;code&gt;-rfunction&lt;/code&gt; option indicates that the GDB &lt;em&gt;skip&lt;/em&gt; machinery should attempt to match functions at which it might stop against the specified regular expression.&lt;/p&gt; &lt;p&gt;In this case, the regular expression is &lt;code&gt;^std::function.*&lt;/code&gt;. The carat (&lt;code&gt;^&lt;/code&gt;) matches the beginning of the string being matched, followed by the literal &lt;code&gt;std::function&lt;/code&gt;. Finally, the&lt;code&gt;.*&lt;/code&gt; matches the rest of the string. (If you want to know more about regular expressions, I wholeheartedly recommend Jeffrey Friedl's book, &lt;a href="https://www.oreilly.com/library/view/mastering-regular-expressions/0596528124/"&gt;Mastering Regular Expressions&lt;/a&gt;.)&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) skip -rfunction ^std::function.* Function(s) ^std::function.* will be skipped when stepping. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, with the &lt;code&gt;skip&lt;/code&gt; in place, let's try the &lt;code&gt;step&lt;/code&gt; again:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) step apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 13 return fn (arg); (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This time, due to the use of the &lt;code&gt;skip&lt;/code&gt; command, shown above, we're able to &lt;code&gt;step&lt;/code&gt; into &lt;code&gt;apply1&lt;/code&gt; in a similar fashion as shown for &lt;code&gt;apply0&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Attempting to step into the function call in apply1&lt;/h2&gt; &lt;p&gt;In attempting to step into the function call on line 13 (shown above), we'll encounter another "gotcha." So let's create another checkpoint:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) checkpoint checkpoint 2: fork returned pid 4193641. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, let's see what happens when we step:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) step 14 }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This didn't step into the successor function as expected.  It turns out that the skip command that we used earlier to avoid seeing the call to the function object constructor is now working against us.  When, while stepping, GDB finds a function to skip, it also skips all functions called by the function in question, even if some descendant call is to a function which wouldn't have been otherwise skipped. To better see what's gone wrong, let's restart at the most recent checkpoint and then disable the skip using the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Skipping-Over-Functions-and-Files.html#index-skip-disable"&gt;&lt;code&gt;skip disable&lt;/code&gt; command&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) restart 2 Switching to Thread 0x7ffff7a89400 (LWP 4193641) #0 apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 13 return fn (arg); (gdb) info skip Num Enb Glob File RE Function 1 y n &lt;none&gt; y ^std::function.* (gdb) skip disable 1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also shown above, is the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Skipping-Over-Functions-and-Files.html#index-info-skip"&gt;&lt;code&gt;info skip&lt;/code&gt; command&lt;/a&gt;, which is used to display a list of the things to be skipped while stepping.&lt;/p&gt; &lt;p&gt;Now, with the &lt;code&gt;skip&lt;/code&gt; on methods in &lt;code&gt;std::function&lt;/code&gt; disabled, let's try stepping again:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) step std::function&lt;int (int)&gt;::operator()(int) const (this=0x7fffffffdb90, __args#0=4) at /usr/include/c++/12/bits/std_function.h:589 589 if (_M_empty())&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We've ended up in the implementation of &lt;code&gt;operator()(int)&lt;/code&gt; in &lt;code&gt;std::function&lt;/code&gt;. While it is possible to end up in &lt;code&gt;successor()&lt;/code&gt; after stepping some more, this is tedious. It is better to instead use the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Set-Breaks.html#index-break"&gt;&lt;code&gt;break&lt;/code&gt; command&lt;/a&gt; to set a breakpoint in &lt;code&gt;successor&lt;/code&gt; and continue:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break successor Breakpoint 3 at 0x40115d: file lambda.cc, line 4. (gdb) continue Continuing. Breakpoint 3, successor (i=4) at lambda.cc:4 4 int successor (int i) { return i + 1; } (gdb) print i $2 = 4 (gdb) backtrace #0 successor (i=4) at lambda.cc:4 #1 0x00000000004026d8 in std::__invoke_impl&lt;int, int (*&amp;)(int), int&gt; ( __f=@0x7fffffffdb90: 0x401156 &lt;successor(int)&gt;) at /usr/include/c++/12/bits/invoke.h:61 #2 0x00000000004025ac in std::__invoke_r&lt;int, int (*&amp;)(int), int&gt; ( __fn=@0x7fffffffdb90: 0x401156 &lt;successor(int)&gt;) at /usr/include/c++/12/bits/invoke.h:114 #3 0x0000000000402452 in std::_Function_handler&lt;int (int), int (*)(int)&gt;::_M_invoke(std::_Any_data const&amp;, int&amp;&amp;) (__functor=..., __args#0=@0x7fffffffdae4: 4) at /usr/include/c++/12/bits/std_function.h:290 #4 0x0000000000402260 in std::function&lt;int (int)&gt;::operator()(int) const ( this=0x7fffffffdb90, __args#0=4) at /usr/include/c++/12/bits/std_function.h:591 #5 0x00000000004011a1 in apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 #6 0x00000000004012d1 in main (argc=1, argv=0x7fffffffdd88) at lambda.cc:25&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note how much more complicated this backtrace is compared to that of &lt;code&gt;apply0&lt;/code&gt; and &lt;code&gt;successor&lt;/code&gt;. This is due to the fact that frames corresponding to calls to C++ support functions for invoking a function object are still on the stack.&lt;/p&gt; &lt;h2&gt;Stopping in a lambda expression&lt;/h2&gt; &lt;p&gt;Thus far, we've encountered difficulties when stepping into a function to which a function object is passed. That problem is easily avoided by either using &lt;code&gt;finish&lt;/code&gt; followed by (another) &lt;code&gt;step&lt;/code&gt; or by using GDB's &lt;code&gt;skip&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;We've also seen that stepping into a call of a function object is problematic - instead of stepping directly into the function object, we instead step into some C++ implementation details related to making such a function call. Assuming we know the function being called, we can simply set a breakpoint on it and then continue.&lt;/p&gt; &lt;p&gt;This is the approach that should be taken for debugging a C++ lambda expression: set a breakpoint on it. Due to its anonymous nature, you don't know its name, so, instead, you must set the breakpoint via its line number. But this too, might be somewhat surprising. Let's take a closer look by placing a breakpoint on the lambda expression on line 33. As a reminder, the code looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt; printf ("Answer 5 is %d\n", apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is what happens when we set a breakpoint on line 33:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break 33 Breakpoint 4 at 0x40124f: /home/kev/examples/lambda.cc:33. (2 locations) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the message indicates that breakpoint 4 has been set at 2 locations. We'll use the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Set-Breaks.html#index-info-breakpoints"&gt;&lt;code&gt;info breakpoints&lt;/code&gt; command&lt;/a&gt; (abbreviated to &lt;code&gt;info break&lt;/code&gt;) to find out more:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) info break 4 Num Type Disp Enb Address What 4 breakpoint keep y &lt;MULTIPLE&gt; 4.1 y 0x000000000040124f in operator()(int) const at lambda.cc:33 4.2 y 0x000000000040136b in main(int, char**) at lambda.cc:33 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This shows the two locations upon which the breakpoint has been set. Breakpoint number 4.2 is in &lt;code&gt;main&lt;/code&gt; and will be the breakpoint that is hit first. The other breakpoint, indicated by 4.1, is the breakpoint for the lambda expression. Let's see what happens when we continue first to 4.2 and then to 4.1, and then look at some program state.  Note that we'll need to &lt;code&gt;continue&lt;/code&gt; twice, stopping once in &lt;code&gt;main&lt;/code&gt;, and the second time in the lambda function.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) continue Continuing. Answer 2 is 5 Answer 3 is 2 Answer 4 is 3 Breakpoint 4.2, main (argc=1, argv=0x7fffffffdd98) at lambda.cc:33 33 apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */ (gdb) continue Continuing. Breakpoint 4.1, operator() (__closure=0x7fffffffdc00, i=4) at lambda.cc:33 33 apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */ (gdb) print i $2 = 4 (gdb) print n $3 = 7 (gdb) backtrace #0 operator() (__closure=0x7fffffffdc00, i=4) at lambda.cc:33 #1 0x0000000000401ff0 in std::__invoke_impl&lt;int, main(int, char**)::&lt;lambda(int)&gt;&amp;, int&gt;(std::__invoke_other, struct {...} &amp;) (__f=...) at /usr/include/c++/12/bits/invoke.h:61 #2 0x0000000000401d3e in std::__invoke_r&lt;int, main(int, char**)::&lt;lambda(int)&gt;&amp;, int&gt;(struct {...} &amp;) (__fn=...) at /usr/include/c++/12/bits/invoke.h:114 #3 0x0000000000401954 in std::_Function_handler&lt;int(int), main(int, char**)::&lt;lambda(int)&gt; &gt;::_M_invoke(const std::_Any_data &amp;, int &amp;&amp;) (__functor=..., __args#0=@0x7fffffffdaf4: 4) at /usr/include/c++/12/bits/std_function.h:290 #4 0x0000000000402260 in std::function&lt;int (int)&gt;::operator()(int) const ( this=0x7fffffffdc00, __args#0=4) at /usr/include/c++/12/bits/std_function.h:591 #5 0x00000000004011a1 in apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 #6 0x0000000000401398 in main (argc=1, argv=0x7fffffffdd98) at lambda.cc:32 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that there are four stack frames in between &lt;code&gt;apply1&lt;/code&gt; and the lambda function at frame #0. This is the C++ support code responsible for invoking the lambda expression. Also note that GDB is able print both the argument &lt;code&gt;i&lt;/code&gt; and the captured variable &lt;code&gt;n&lt;/code&gt; which was declared in &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Stopping in a particular invocation of a lambda expression&lt;/h2&gt; &lt;p&gt;The example program defines a function which returns a function object representing a lambda expression:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;std::function&lt;int(int)&gt; make_function(int&amp; x) { return [&amp;] (int i) { return i + x; }; /* Line 17 */ } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's used twice as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; auto lf2 = make_function (n); printf ("Answer 6 is %d\n", apply1 (lf2, 1)); /* Line 36 */ auto lf3 = make_function (m); printf ("Answer 7 is %d\n", apply1 (lf3, -14)); /* Line 39 */ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Suppose that we wish to only stop in the lambda expression invoked on line 39. The lambda expression is actually on line 17, so we'll have to place a breakpoint on that line, but, as we'll see, the code for the lambda expression is also invoked via the call to apply1 on line 36. We'll arrive at the solution in stages.&lt;/p&gt; &lt;p&gt;We can start out by placing a breakpoint on line 17, after first making another checkpoint. (I'm making another checkpoint so that we can easily back up to an earlier execution point without having to start the program from scratch.) Also, we'll take a look at the locations at which GDB has placed breakpoints:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) checkpoint checkpoint 3: fork returned pid 2030341. (gdb) break 17 Breakpoint 5 at 0x4011af: /home/kev/examples/lambda.cc:17. (2 locations) (gdb) info breakpoint 5 Num Type Disp Enb Address What 5 breakpoint keep y &lt;MULTIPLE&gt; 5.1 y 0x00000000004011af in operator()(int) const at lambda.cc:17 5.2 y 0x00000000004011cf in make_function(int&amp;) at lambda.cc:17 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;info breakpoint 5&lt;/code&gt; command shows that breakpoint 5.2 is in &lt;code&gt;make_function&lt;/code&gt; and that breakpoint 5.1 is in &lt;code&gt;operator()(int)&lt;/code&gt;, which is the lambda expression. Our goal is to stop only in the lambda expression, so let's disable breakpoint 5.2.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) disable 5.2 (gdb) info breakpoint 5 Num Type Disp Enb Address What 5 breakpoint keep y &lt;MULTIPLE&gt; 5.1 y 0x00000000004011af in operator()(int) const at lambda.cc:17 5.2 n 0x00000000004011cf in make_function(int&amp;) at lambda.cc:17 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now let's see what happens when we &lt;code&gt;continue&lt;/code&gt;.  It'll take two &lt;code&gt;continue&lt;/code&gt; commands to get to the lambda invoked by the &lt;code&gt;apply1&lt;/code&gt; call at line 39:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) continue Continuing. Answer 5 is 11 Breakpoint 5.1, operator() (__closure=0x7fffffffdc20, i=1) at lambda.cc:17 17 return [&amp;] (int i) { return i + x; }; /* Line 17 */ (gdb) continue Continuing. Answer 6 is 8 Breakpoint 5.1, operator() (__closure=0x7fffffffdc40, i=-14) at lambda.cc:17 17 return [&amp;] (int i) { return i + x; }; /* Line 17 */ (gdb) backtrace #0 operator() (__closure=0x7fffffffdc40, i=-14) at lambda.cc:17 #1 0x0000000000401e70 in std::__invoke_impl&lt;int, make_function(int&amp;)::&lt;lambda(int)&gt;&amp;, int&gt;(std::__invoke_other, struct {...} &amp;) (__f=...) at /usr/include/c++/12/bits/invoke.h:61 #2 0x0000000000401a79 in std::__invoke_r&lt;int, make_function(int&amp;)::&lt;lambda(int)&gt;&amp;, int&gt;(struct {...} &amp;) (__fn=...) at /usr/include/c++/12/bits/invoke.h:114 #3 0x000000000040174e in std::_Function_handler&lt;int(int), make_function(int&amp;)::&lt;lambda(int)&gt; &gt;::_M_invoke(const std::_Any_data &amp;, int &amp;&amp;) (__functor=..., __args#0=@0x7fffffffdae4: -14) at /usr/include/c++/12/bits/std_function.h:290 #4 0x0000000000402260 in std::function&lt;int (int)&gt;::operator()(int) const ( this=0x7fffffffdc40, __args#0=-14) at /usr/include/c++/12/bits/std_function.h:591 #5 0x00000000004011a1 in apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=-14) at lambda.cc:13 #6 0x0000000000401452 in main (argc=1, argv=0x7fffffffdd88) at lambda.cc:39&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;backtrace&lt;/code&gt; shows that the second &lt;code&gt;continue&lt;/code&gt; command brought us to the lambda expression invoked by the the call to &lt;code&gt;apply1&lt;/code&gt; at line 39 in &lt;code&gt;main&lt;/code&gt;. For this simple program, it's not onerous to continue twice, but in real application code, it might happen that hundreds or thousands of &lt;code&gt;continue&lt;/code&gt; commands might be needed to stop at the point of interest.&lt;/p&gt; &lt;p&gt;To show how we can stop at a breakpoint which might be hit after some other line of code has been executed, let's first restart from checkpoint 3:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) restart 3 Switching to Thread 0x7ffff7a89400 (LWP 2030341) #0 operator() (__closure=0x7fffffffdbf0, i=4) at lambda.cc:33 33 apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This causes GDB to switch to the fork created in the previous checkpoint, but it doesn't undo any of the breakpoints that we set up after creating the checkpoint. To see this, let's look again at breakpoint 5:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) info breakpoint 5 Num Type Disp Enb Address What 5 breakpoint keep y &lt;MULTIPLE&gt; breakpoint already hit 2 times 5.1 y 0x00000000004011af in operator()(int) const at lambda.cc:17 5.2 n 0x00000000004011cf in make_function(int&amp;) at lambda.cc:17 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is pretty close to the state of breakpoint 5 when we last looked at it. One difference is that it tells us that breakpoint 5 has (overall) been hit 2 times, whereas that message was missing we we looked at it earlier.&lt;/p&gt; &lt;p&gt;Now, just so it's clear where we are in the program, we'll set a temporary breakpoint at line 35 and then continue:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) tbreak 35 Temporary breakpoint 6 at 0x4013b5: file lambda.cc, line 35. (gdb) continue Continuing. Answer 5 is 11 Temporary breakpoint 6, main (argc=1, argv=0x7fffffffdd88) at lambda.cc:35 35 auto lf2 = make_function (n);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GDB has stopped on the first call to &lt;code&gt;make_function&lt;/code&gt;. The following line, 36, will call &lt;code&gt;apply1&lt;/code&gt; using the function object held in &lt;code&gt;lf2&lt;/code&gt;. Recall that it's our goal to stop in the lambda expression invoked by calling &lt;code&gt;apply1&lt;/code&gt; on line 39. In order to realize that goal, let's first disable breakpoint 5 and then look at what &lt;code&gt;info breakpoint&lt;/code&gt; says about it:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) disable 5 (gdb) info break 5 Num Type Disp Enb Address What 5 breakpoint keep n &lt;MULTIPLE&gt; breakpoint already hit 2 times 5.1 y- 0x00000000004011af in operator()(int) const at lambda.cc:17 5.2 n 0x00000000004011cf in make_function(int&amp;) at lambda.cc:17&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Examining the 'Enb' column shows that, overall, breakpoint 5 is disabled, but were it to be enabled, then breakpoint 5.1 would also be enabled, while breakpoint 5.2 would still be disabled. If we were to continue at this point, the program would not stop at either of the breakpoint 5 locations since it is currently disabled.&lt;/p&gt; &lt;p&gt;The next step is to place a breakpoint at line 39 and then add some commands to run when that breakpoint is hit.  Specifically, we'll enable breakpoint 5 as one of the commands. The &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Break-Commands.html#index-commands"&gt;&lt;code&gt;commands&lt;/code&gt; command&lt;/a&gt; is used to associate some GDB commands with a breakpoint; the &lt;code&gt;commands&lt;/code&gt; command can be given an argument specifying the breakpoint number (to which to associate some GDB commands), but when no breakpoint number is specified, as is shown below, it simply attaches commands to the most recently created breakpoint.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break 39 Breakpoint 7 at 0x40142b: file lambda.cc, line 39. (gdb) commands Type commands for breakpoint(s) 7, one per line. End with a line saying just "end". &gt;enable 5 &gt;continue &gt;end&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After enabling breakpoint 5 (via the command &lt;code&gt;enable 5&lt;/code&gt;), a &lt;code&gt;continue&lt;/code&gt; command will be issued. Thus, this breakpoint won't stop for user interaction, but will continue execution after first enabling breakpoint #5, which is for the lambda expression that we want to stop in. This is what happens when we continue:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) continue Continuing. Answer 6 is 8 Breakpoint 7, main (argc=1, argv=0x7fffffffdd88) at lambda.cc:39 39 printf ("Answer 7 is %d\n", apply1 (lf3, -14)); /* Line 39 */ Breakpoint 5.1, operator() (__closure=0x7fffffffdc40, i=-14) at lambda.cc:17 17 return [&amp;] (int i) { return i + x; }; /* Line 17 */ (gdb) print x $5 = (int &amp;) @0x7fffffffdb88: -28 (gdb) print i $6 = -14 (gdb) backtrace #0 operator() (__closure=0x7fffffffdc40, i=-14) at lambda.cc:17 #1 0x0000000000401e70 in std::__invoke_impl&lt;int, make_function(int&amp;)::&lt;lambda(int)&gt;&amp;, int&gt;(std::__invoke_other, struct {...} &amp;) (__f=...) at /usr/include/c++/12/bits/invoke.h:61 #2 0x0000000000401a79 in std::__invoke_r&lt;int, make_function(int&amp;)::&lt;lambda(int)&gt;&amp;, int&gt;(struct {...} &amp;) (__fn=...) at /usr/include/c++/12/bits/invoke.h:114 #3 0x000000000040174e in std::_Function_handler&lt;int(int), make_function(int&amp;)::&lt;lambda(int)&gt; &gt;::_M_invoke(const std::_Any_data &amp;, int &amp;&amp;) (__functor=..., __args#0=@0x7fffffffdae4: -14) at /usr/include/c++/12/bits/std_function.h:290 #4 0x0000000000402260 in std::function&lt;int (int)&gt;::operator()(int) const ( this=0x7fffffffdc40, __args#0=-14) at /usr/include/c++/12/bits/std_function.h:591 #5 0x00000000004011a1 in apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=-14) at lambda.cc:13 #6 0x0000000000401452 in main (argc=1, argv=0x7fffffffdd88) at lambda.cc:39&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that GDB shows that breakpoint 7 was hit, but it doesn't stop at it. If we wanted to cause that message to not be printed, we could have used the special command &lt;code&gt;silent&lt;/code&gt; as the first command in the commands for breakpoint #7. (Note: the &lt;code&gt;silent&lt;/code&gt; command can only be used as a command within the &lt;code&gt;commands&lt;/code&gt; command.)&lt;/p&gt; &lt;p&gt;The above example also prints the value of the captured variable &lt;code&gt;x&lt;/code&gt; and the argument (to the lambda function) &lt;code&gt;i&lt;/code&gt;. Frame #6 of the backtrace shows that the call to &lt;code&gt;apply1&lt;/code&gt; was invoked from line 39 in function &lt;code&gt;main&lt;/code&gt;. Frames #0, #5, and #6 correspond to lines of code in our example program. The remaining frames, #1 thru #4, show calls to functions within the C++ library.&lt;/p&gt; &lt;p&gt;GDB's ability to execute commands associated with a breakpoint is a powerful feature that's useful in many other situations as well.&lt;/p&gt; &lt;h2&gt;&lt;a id="Debugging an LLVM compiled program" name="Debugging an LLVM compiled program"&gt;&lt;/a&gt;Debugging an LLVM-compiled program&lt;/h2&gt; &lt;p&gt;As noted earlier, the interactions with GDB shown above were performed against an executable built with GCC 12.1.&lt;/p&gt; &lt;p&gt;If you're using clang++ instead of the GNU compiler, most of the interactions will be similar if not identical to that shown above. One somewhat important difference is the order in which the locations for a breakpoint on a line containing a lambda expression are shown. When using the LLVM compiler, these locations are reversed from that shown for the GNU compiler. E.g., when setting a breakpoint on line 33, we (might) see this instead when debugging an executable produced by the LLVM compiler:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break 33 Breakpoint 4 at 0x4013bb: /home/kev/examples/lambda.cc:33. (2 locations) (gdb) info break 4 Num Type Disp Enb Address What 4 breakpoint keep y &lt;MULTIPLE&gt; 4.1 y 0x00000000004013bb in main(int, char**) at lambda.cc:33 4.2 y 0x000000000040205f in main::$_2::operator()(int) const at lambda.cc:33&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This isn't especially important for breakpoint 4, but it is important for breakpoint 5 which was placed on line 17. Before, when using the GCC executable, we disabled the breakpoint on 5.2 in order to disable the breakpoint on the outer scope. But, when using an LLVM executable, we instead have to disable the breakpoint on 5.1 instead. I.e.:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) disable 5.1 (gdb) info breakpoint 5 Num Type Disp Enb Address What 5 breakpoint keep y &lt;MULTIPLE&gt; 5.1 n 0x00000000004011f7 in make_function(int&amp;) at lambda.cc:17 5.2 y 0x000000000040195f in make_function(int&amp;)::$_0::operator()(int) const at lambda.cc:17&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This could also happen with some past or future version of the GNU compiler. You shouldn't assume that you know the order of the locations, but should instead use the &lt;code&gt;info breakpoints&lt;/code&gt; command to figure out which breakpoint location to disable or even delete.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;This article has discussed several problems that might encountered when using GDB to debug function objects. These problems include inadvertently stepping into constructors as well as the related problem of stepping into function object invocation code. In order to avoid the latter problem, it is best to place a breakpoint in the target function. This can be done by name, assuming we know the name, but in the case of lambda expressions, there is no name, so the breakpoint must be placed via line number.&lt;/p&gt; &lt;p&gt;When placing a breakpoint on a line containing a lambda, it's frequently the case that the breakpoint will be placed at multiple locations. The &lt;code&gt;info breakpoints&lt;/code&gt; command can be used to determine which location is that of the containing function and which is the lambda; once this is determined, GDB's &lt;code&gt;disable&lt;/code&gt; command can be used to cause GDB to not stop at one of those locations.&lt;/p&gt; &lt;p&gt;Finally, a helper breakpoint with associated commands can be used to re-enable a breakpoint that's been disabled. The combination of first disabling a breakpoint on a lambda and then using a helper breakpoint to re-enable the lambda's breakpoint is useful for targeting a specific invocation of a lambda expression or other function object.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/03/how-debug-c-lambda-expressions-gdb" title="How to debug C++ lambda expressions with GDB"&gt;How to debug C++ lambda expressions with GDB&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Kevin Buettner</dc:creator><dc:date>2023-05-03T07:00:00Z</dc:date></entry><entry><title>How to set up event-driven microservices using Knative Eventing</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/02/how-set-event-driven-microservices-using-knative-eventing" /><author><name>Matthias Wessendorf</name></author><id>0244c1a3-a6bf-4876-9c57-26b9ab99e820</id><updated>2023-05-02T07:00:00Z</updated><published>2023-05-02T07:00:00Z</published><summary type="html">&lt;p&gt;Many modern application designs are &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven&lt;/a&gt;, aiming to deliver events quickly. This article describes how to orchestrate event-driven &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt; using standards like CNCF CloudEvents and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; APIs for Knative to simplify EDA-style application development.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://www.redhat.com/en/topics/integration/what-is-event-driven-architecture"&gt;Event Driven Architecture (EDA)&lt;/a&gt; allows the implementation of loosely coupled applications and services. In this model, event producers do not know for which event consumers are listening, and the event itself does not know the consequences of its occurrence. EDA is a good option for distributed application architectures.&lt;/p&gt; &lt;p&gt;Figure 1 illustrates an example of an event-driven application consisting of three services, producing and consuming different events with an event bus responsible for the orchestration and routing of the events. Note that “Service 3” produces an event indicating the business process finished, but there is no consumer for the event.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-02-27_18-13-39.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-02-27_18-13-39.png?itok=Z1ss7cix" width="600" height="515" alt="Illustration of an event-driven application consisting of three services, producing and consuming different events." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: An event-driven application consisting of three services, producing and consuming different events.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;It is simple to implement another consumer for the “Finish Event” with the flexible architecture of event-driven systems.&lt;/p&gt; &lt;h2&gt;Knative Eventing&lt;/h2&gt; &lt;p&gt;How can you build a system on Kubernetes that orchestrates events and routes them to consumers? Luckily there is &lt;a href="https://knative.dev/docs/eventing"&gt;Knative Eventing&lt;/a&gt;, which offers a collection of APIs that enable cloud native developers to use an event-driven architecture within their applications and services. You can use these APIs to create components that route events from event producers to event consumers, known as sinks, that receive events.&lt;/p&gt; &lt;p&gt;Knative Eventing uses standard HTTP requests to send and receive events between event producers and sinks. These events conform to a CNCF industry standard called &lt;a href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt;, which enables creating, parsing, sending, and receiving events in any programming language. The binding between the HTTP protocol and CloudEvents is standardized in this &lt;a href="https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/http-protocol-binding.md"&gt;specification&lt;/a&gt;. Although the focus of this article is on HTTP, it is worth mentioning that the CloudEvents specification also describes bindings for &lt;a href="https://github.com/cloudevents/spec/tree/v1.0.2/cloudevents/bindings"&gt;other protocols&lt;/a&gt;, such as &lt;a href="http://docs.oasis-open.org/amqp/core/v1.0/os/amqp-core-overview-v1.0-os.html"&gt;AMQP&lt;/a&gt; or &lt;a href="https://datatracker.ietf.org/doc/html/rfc6455"&gt;Websocket&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Event mesh with Knative Broker&lt;/h2&gt; &lt;p&gt;One of the key APIs in Knative Eventing is the &lt;a href="https://knative.dev/docs/eventing/brokers"&gt;Knative Broker API&lt;/a&gt;, which defines an &lt;a href="https://knative.dev/docs/eventing/event-mesh/"&gt;event mesh&lt;/a&gt; aiding the event orchestration and routing. Figure 2 shows a complete process, covering purchases from an online web shop and ends when the order is completely delivered at the customer's door. The process is implemented by a couple of microservice applications that are consuming and producing events. There is no direct communication or invocation between the services. Instead, the applications are loosely coupled and communicate only via events.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screenshot%20from%202023-03.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Screenshot%20from%202023-03.png?itok=Uj_tWe6i" width="600" height="272" alt="An illustration of the process flow of an event-driven application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The complete process flow of an event-driven application.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The orchestration of the event exchange is handled by an event mesh. In our case, this is the Knative Broker for &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: eventing.knative.dev/v1 kind: Broker metadata:  annotations:    eventing.knative.dev/broker.class: Kafka  name: order-broker spec:  config:    apiVersion: v1    kind: ConfigMap    name: order-broker-config --- apiVersion: v1 kind: ConfigMap metadata:  name: order-broker-config data:  bootstrap.servers: &lt;url&gt;  auth.secret.ref.name: &lt;optional-secret-name&gt;  default.topic.partitions: "10"  default.topic.replication.factor: "3"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The broker is annotated to pick the Kafka-backed implementation and points to a ConfigMap holding configuration about the Apache Kafka Topic, internally used by the broker. Generally, it is also recommended to configure aspects of delivery guarantees and retries. But we skipped this to keep this article simple. For more details on best practices for Knative Broker configurations, read the article, &lt;a href="https://developers.redhat.com/articles/2023/03/08/configuring-knative-broker-apache-kafka"&gt;Our advice for configuring Knative Broker for Apache Kafka&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;After the Broker definition has been applied with &lt;code&gt;oc apply&lt;/code&gt;, you can check for the broker and its status as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get brokers.eventing.knative.dev -n orders NAME           URL                                                                                 AGE   READY order-broker   http://kafka-broker-ingress.knative-eventing.svc.cluster.local/orders/order-broker   46m   True&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Every Knative Broker object exposes an HTTP endpoint acting as the Ingress for CloudEvents. The URL can be found on the status of each broker object. The following is an example of an HTTP POST request that could be sent from the “Online Shop” service to the event mesh, aka the Knative Kafka Broker.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -v -X POST \ -H "content-type: application/json" \ -H "ce-specversion: 1.0" \ -H "ce-source: /online/shop" \ -H "ce-type: order.requested" \ -H "ce-id: 1f1380d4-8ff2-4ab0-b2ba-54811226c21b" \ -d '{"customerId": 20207-19, "orderId": "f8bc3445-b844"}' \ http://kafka-broker-ingress.knative-eventing.svc.cluster.local/orders/order-broker&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But how is the event getting delivered to the “Payment Service”, since there is no direct coupling between the two?&lt;/p&gt; &lt;h2&gt;Event orchestration and routing&lt;/h2&gt; &lt;p&gt;While the Broker API implements an event mesh, it goes hand-in-hand with the trigger API, which the broker is using to route messages based on a given set of rules or criteria to their destination.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: trigger-order-requested spec: broker: order-broker filter: attributes: type: order.requested source: /online/shop subscriber: ref: apiVersion:v1 kind: Service name: payment&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This example is a trigger for the “order-broker” which contains two filters, each for different CloudEvent attributes metadata:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Type&lt;/li&gt; &lt;li aria-level="1"&gt;Source&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Both rules are treated as an AND, and the event is only routed to the referenced payment service by the Knative Broker if both rules match.&lt;/p&gt; &lt;p&gt;The routing of the matching CloudEvent to the referenced Kubernetes Service (payment) is done by the Knative Broker using HTTP. This allows a flexible architecture for the implementation of the processing services since simply all that is needed is a Web Server program, regardless of the written language. Besides the Kubernetes Service API, we can also reference a &lt;a href="https://knative.dev/docs/serving/"&gt;Knative Serving Service&lt;/a&gt;, supporting serverless principals.&lt;/p&gt; &lt;p&gt;If the referenced service replies with a CloudEvent in its HTTP response, this event is returned back to the Knative Broker and available for further processing. Using a different trigger with a matching rule can route those events to other service applications.&lt;/p&gt; &lt;h2&gt;CloudEvent processing with Knative Functions&lt;/h2&gt; &lt;p&gt;One simple way to create microservices that are processing standard CloudEvents is to leverage the &lt;a href="https://knative.dev/docs/functions/"&gt;Knative Functions&lt;/a&gt; project. It contains templates for a number of languages and platforms, such as:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/go"&gt;Golang&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; (&lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt;)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; (&lt;a href="https://developers.redhat.com/node/219015"&gt;Java&lt;/a&gt;)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring Boot&lt;/a&gt; (Java)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/rust"&gt;Rust&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The following script is the implementation of the “Payment Service” application, which was written based on the &lt;a href="https://quarkus.io/guides/funqy"&gt;Quarkus Funqy&lt;/a&gt; template. Funqy is part of Quarkus’s serverless strategy that provides a portable Java API for developers to write serverless functions and deploy them to heterogeneous serverless runtimes, including AWS Lambda, Azure Functions, Google Cloud, and Knative. With Funqy, developers can easily bind their methods to CloudEvents, using the @Funq annotation. Funqy ensures that the @Funq annotated Java method is invoked with the HTTP request from the Knative Broker, containing the “OrderRequested” CloudEvent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The OrderRequests object is part of the CloudEvent payload in a serialized JSON format, as indicated by the previous cURL example.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;@Funq public CloudEvent&lt;PaymentReceived&gt; orderRequest(final CloudEvent&lt;OrderRequested&gt; order) { LOG.debug("Incoming CloudEvent with ID: " + order.id()); try { final PaymentReceived payment = paymentProvider.processPayment(order.data()); return CloudEventBuilder.create() .id(UUID.randomUUID().toString()) .type("payment.received") .source("/payment") .build(payment); } catch (InvalidPaymentException ipe) { // recover from here return CloudEventBuilder.create().build(...); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The CloudEvent payload is deserialized to the “OrderRequest” type, using the &lt;code&gt;data()&lt;/code&gt; method from the CloudEvents API and processed by a payment provider service. Once the payment is approved, the Knative Function code returns a different CloudEvent with type &lt;strong&gt;payment.received&lt;/strong&gt;, indicating the payment has been received.&lt;/p&gt; &lt;p&gt;Let’s have a look at the diagram in Figure 2 where the “Order Service” subscribed to the “payment.received” event. Whenever such an event is available in the event mesh, the Knative Broker will dispatch it to the subscribed “Order Service” and continue the process of the shopping cart application.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In case of a failure, we see the &lt;strong&gt;InvalidPaymentException&lt;/strong&gt; and a different CloudEvent with an error type returned to the broker, indicating that a failure has occurred. For more information on how to configure the Knative Broker for delivery guarantees and retries, please refer to the previously mentioned &lt;a href="https://developers.redhat.com/articles/2023/03/08/configuring-knative-broker-apache-kafka"&gt;article&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Using application-specific events&lt;/h3&gt; &lt;p&gt;When working with event-driven microservices, it is highly recommended that every service or function should respond to incoming requests with an outgoing event on its HTTP response. The CloudEvents should be domain-specific and provide context about their state on the CloudEvent metadata attributes. It is very important to not return the same CloudEvent type that goes into a function because this would cause a filter loop on the executing Knative Broker. For successful event processing and domain-specific failures, a service should always return a CloudEvent to the Knative Broker.&lt;/p&gt; &lt;p&gt;To handle network-level failures occurring while the Knative Broker tries to deliver the CloudEvents (such as HTTP 4xx/5xx errors), we recommend configuring a Dead-Letter-Sink for improved delivery guarantees.&lt;/p&gt; &lt;h2&gt;Knative Eventing simplifies event-driven microservices&lt;/h2&gt; &lt;p&gt;The article described how the Knative Eventing Broker and Trigger APIs help to orchestrate event-driven microservices. The architecture is based on standardized Kubernetes APIs for Knative and CNCF CloudEvents. We also discussed how the implementation of EDA-style applications are loosely coupled and how to implement a simple routing approach for events using Knative Eventing. Leveraging industry standards is a good investment for any application architecture. If you have questions, please comment below. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/02/how-set-event-driven-microservices-using-knative-eventing" title="How to set up event-driven microservices using Knative Eventing"&gt;How to set up event-driven microservices using Knative Eventing&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Matthias Wessendorf</dc:creator><dc:date>2023-05-02T07:00:00Z</dc:date></entry><entry><title type="html">New Keycloak maintainer: Sebastian Schuster</title><link rel="alternate" href="https://www.keycloak.org/2023/05/maintainer-sschu" /><author><name>Stian Thorgersen</name></author><id>https://www.keycloak.org/2023/05/maintainer-sschu</id><updated>2023-05-02T00:00:00Z</updated><content type="html">We are pleased to welcome as an official maintainer of Keycloak. Sebastian has contributed to Keycloak since 2019, when he convinced his company Bosch to use Keycloak for identity and access management. He has been active in the community providing help, taking part in discussions and contributing. Behind him, there is a whole team at Bosch providing more than 60 contributions over the last years in various areas. The declarative user profile was the most prominent feature contributed. His company allows him to dedicate a considerable amount of time for Keycloak to help review contributions and reports and get involved in discussions. Since Sebastian has got experience operating Keycloak on a wide scale over several years, he will focus on topics around cloud-native and Keycloak operations like observability. Not only will Sebastian on his own bring a lot of value to Keycloak, but he will also serve as an integration point for Bosch to enable more contributions from his team, allowing them to contribute more value to Keycloak in the future.</content><dc:creator>Stian Thorgersen</dc:creator></entry></feed>
